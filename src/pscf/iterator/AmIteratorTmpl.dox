namespace Pscf{
/*! 

\page pscf_AmIteratorTmpl_page Anderson Mixing Iteration

Many of the SCFT iteration algorithms and PS-FTS compressor algorithms
used by PSCF are based on some variant the Anderson mixing algorithm.
Anderson mixing (AM) is a general stategy for iteratively solving a 
nonlinear system of equations by taking advantage of information obtained 
during several previous iterations. PSCF C++ classes that use this 
strategy are all derived from a class template named Pscf::AmIteratorTmpl
that provides data structures and algorithms required in any variant of
this strategy. 

\section pscf_AmIteratorTmpl_sec Anderson Mixing Algorithms

Consider a system of \f$ N \f$ nonlinear equations that must be solved 
for \f$ N \f$ variables. Let \f$ {\bf X} \f$ denote a vector of unknown 
variables with elements \f$  X_{0}, \ldots, X_{N-1} ) \f$. 
We will refer to this vector of variables as a state vector.  We assume 
without loss of generality that the problem is expressed as a system of 
equations of the form
\f[
  R_{i}({\bf X}) = 0
\f]
for all integer \f$ i = 0, \ldots, N-1 \f$. Here, \f$R_{i} \f$ denotes 
a single element of a residual vector \f$ {\bf R} \f$ with \f$ N \f$
elements \f$ (R_{0}, \ldots, R_{N-1} )\f$, such that each element of 
this vector is a function of the full state vector \f$ {\bf X} \f$. 
This description is intentionally general - the nature of the elements 
of the state and residual vector depend on the nature of the problem. 

To describe a sequence of states generated by an iterative algorithm, 
let \f$ {\bf X}^{k} \f$ and \f${\bf R }^{k} \f$ denote values of the 
state vector and residual vector, respectively, at the end of iteration 
step \f$ k \f$, for \f$ k = 1, 2, \ldots \f$. Let \f$ {\bf X}^{0} \f$ 
and \f${\bf R }^{0} \f$ denote initial values, before the first iteration.

Each step of an AM algorithm consists of two steps, which we refer to 
as projection and correction. The projection step uses information from
previous iterations to produce an intermediate state, and the correction
step attempts to further reduce the error remaining after the projection
step. To describe this, let \f$ \underline{{\bf X}}^{k} \f$ denote the state 
vector obtained after the projection step of iteration \f$ k \f$ but 
before the projection step. 

<b> Projection step </b>:

We first consider the projection step, which uses information from 
previous states. This part of the algorithm is based on the assumption 
that the initial guess is close enough to a solution that the function 
\f${\bf R }({\bf X}) \f$ may be assumed to be nearly linear. For each iteration 
\f$ k > 1 \f$, the projection step uses stored values of values of 
\f$ {\bf X}^{k} \f$ and \f${\bf R }^{k} \f$ at the current states and \f$ P-1 \f$ 
previous states to construct a trial vector \f$ \underline{{\bf X}}^{k} \f$ 
as a linear superposition of the form
\f[
  \underline{{\bf X}}^{k} = {\bf X}^{k-1} + 
  \sum_{j=1}^{P-1} 
  C_j ( {\bf X}^{k-j} - {\bf X}^{k-j-1} ) \quad,
\f]
in which \f$ C_{1}, \ldots, C_{P-1} \f$ are real coefficients that 
will be optimized to minimize the norm an estimate of the resulting
residual vector.  The parameter \f$ P \f$ is set equal to the smaller 
of \f$ k \f$ and a user-defined integer parameter \f$N_{\rm h} \f$ that 
gives the maximum number of stored states.  A predicted value of the 
residual vector in this state, denoted by \f$ \underline{{\bf R } }^{k} \f$, 
is obtained by assuming linearity, giving a predicted residual 
vector
\f[
  \underline{{\bf R } }^{k} ={\bf R }^{k-1} + 
  \sum_{j=1}^{P-1} C_j ({\bf R }^{k-j} -{\bf R }^{k-j-1} ) \quad,
\f]
that is given by a corresponding linear superposition of previous
residual vector values. The values of the coefficients 
\f$ C_{1}, \ldots, C_{P-1} \f$ are chosen so as to minimize the
\f$ l^{2} \f$ norm of the predicted residual \f$ \underline{{\bf R } }^{k} \f$.
This corresponds to minimization of the linearized residual within 
a \f$ P - 1 \f$ dimensional subspace for the difference
\f$ \underline{{\bf R } }^{k} -{\bf R }^{k-1} \f$ that is spanned by a basis of
\f$ P - 1 \f$ differences between values of \f$ {\bf X} \f$ in previous 
states.  We refer to this as "projection" because it yields a change 
in the state vector that has been projected into this relatively low 
dimensional subspace.

The resulting optimization problem is a straightforward exercise in 
linear algebra that is solved within member functions of the class 
template Pscf::AmIteratorTmpl.  This yields a optimal value of 
\f$ \underline{{\bf R } }^{k} \f$ that is orthogonal to the \f$ P -1 \f$ 
dimensional subspace spanned by differences between previous values 
of \f${\bf R } \f$.

<b> Correction </b>:

The correction step adds an additional correction vector, denoted by 
\f$ {\bf D}^{k} \f$, to obtain a new state
\f[
   {\bf X}^{k} = \underline{{\bf X}}^{k} + {\bf D}^{k}
\f]
The value of the correction vector \f$ D^{k} \f$ generally depends upon 
the residual vector \f$ \underline{{\bf R } }^{k} \f$ that is predicted 
to remain after the projection step. The projection vector generally 
does not lie within within the \f$ P - 1 \f$ dimensional subpace in 
which the projection step attempts to minimize the residual.

The simplest version of AM mixing use a correction vector that is 
simply to the predicted residual, giving
\f[
   {\bf D}^{k} = \pm \lambda \underline{{\bf R }}^{k}
\f]
Here, \f$ \lambda \f$ is an adjustable non-negative "mixing" parameter, 
and the sign of the correction depends upon the how the residual vector 
is defined (i.e., whether changes in the residual vector are more nearly 
parallel or anti-parallel to changes in the state vector). 
Some variants of the algorithm used in polymer field theory have also
used a mixing parameter \f$ \lambda \f$ that depends on the iteration 
index index \f$ k \f$, and that starts out very low and increases for 
the first few iterations. 

The design of the template uses a set of 
pure virtual function to define operations such as computation of the 
residual vector for a given state vector, thus allowing this template
to be used as the basis for this algorithm in algorithms designed to
solve several different types of problem, and different possible ways
of defining the residual vector.

*/
}
